{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task Extraction and Categorization using NLP\n",
        "\n",
        "This notebook processes text to extract and categorize task-related information using Natural Language Processing (NLP) techniques.\n",
        "\n",
        "### Steps Included:\n",
        "1. **Preprocessing** - Cleaning text by removing punctuation and stop words.\n",
        "2. **Tokenization & POS Tagging** - Splitting text into sentences and tagging parts of speech.\n",
        "3. **Task Identification** - Extracting task-related sentences.\n",
        "4. **Task Categorization** - Grouping extracted tasks using NLP models.\n",
        "5. **Extracting Task Details** - Identifying responsible persons and deadlines."
      ],
      "metadata": {
        "id": "GiH7HNWAGEsu"
      },
      "id": "GiH7HNWAGEsu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2cf8ea-fe01-4893-b1d2-048cff1e8c30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c2cf8ea-fe01-4893-b1d2-048cff1e8c30",
        "outputId": "d05125ff-15b5-4b2e-acd0-62214b532335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "# Downloading necessary NLP resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba783bb-88b6-445e-8335-6d337c228d81",
      "metadata": {
        "id": "bba783bb-88b6-445e-8335-6d337c228d81"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function cleans the input text by:\n",
        "1. Converting it to lowercase.\n",
        "2. Removing punctuation.\n",
        "3. Removing stop words.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"I clean the input text by removing punctuation and stop words.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Remove punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    cleaned_text = \" \".join([word for word in words if word not in stop_words])  # Remove stop words\n",
        "    return cleaned_text, words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5381e7ea-150b-4de7-94ee-ce6af44549a8",
      "metadata": {
        "id": "5381e7ea-150b-4de7-94ee-ce6af44549a8"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_tag(text):\n",
        "    \"\"\"I split the text into sentences and perform part-of-speech (POS) tagging.\"\"\"\n",
        "    sentences = sent_tokenize(text)  # Split text into sentences\n",
        "    tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in sentences]  # POS tagging\n",
        "    return tagged_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640e932f-b1eb-4804-ad1f-9d5c5c9a7b4a",
      "metadata": {
        "id": "640e932f-b1eb-4804-ad1f-9d5c5c9a7b4a"
      },
      "outputs": [],
      "source": [
        "def identify_tasks(text):\n",
        "    \"\"\"I identify sentences that likely represent tasks based on heuristic rules.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    task_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        # Heuristics for identifying tasks\n",
        "        action_verbs = [word for word, tag in pos_tags if tag.startswith(\"VB\")]\n",
        "        if \"has\" in words or \"must\" in words or \"should\" in words or action_verbs:\n",
        "            task_sentences.append(sentence)\n",
        "\n",
        "    return task_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22195a23-a4f1-44ee-b17c-58843c81422d",
      "metadata": {
        "id": "22195a23-a4f1-44ee-b17c-58843c81422d"
      },
      "outputs": [],
      "source": [
        "def categorize_tasks(task_sentences):\n",
        "    \"\"\"I categorize extracted tasks using Word2Vec and Latent Dirichlet Allocation (LDA).\"\"\"\n",
        "    tokenized_tasks = [word_tokenize(sentence) for sentence in task_sentences]\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    word2vec_model = Word2Vec(tokenized_tasks, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Define useful categories dynamically using LDA topic modeling\n",
        "    dictionary = Dictionary(tokenized_tasks)\n",
        "    corpus = [dictionary.doc2bow(text) for text in tokenized_tasks]\n",
        "    lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
        "\n",
        "    categorized_tasks = {}\n",
        "    for sentence in task_sentences:\n",
        "        bow_vector = dictionary.doc2bow(word_tokenize(sentence))\n",
        "        topic = max(lda_model[bow_vector], key=lambda x: x[1])[0]  # Get dominant topic\n",
        "        categorized_tasks[sentence] = f\"Category {topic+1}\"\n",
        "\n",
        "    return categorized_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1822f4d-e606-47f0-8554-2800f7defc0a",
      "metadata": {
        "id": "c1822f4d-e606-47f0-8554-2800f7defc0a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function extracts details from task-related sentences, such as:\n",
        "1. Responsible person.\n",
        "2. Deadline.\n",
        "\"\"\"\n",
        "def extract_task_details(task_sentences):\n",
        "    \"\"\"I extract the responsible person and deadline from task sentences.\"\"\"\n",
        "    task_details = []\n",
        "    for sentence in task_sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        person = None\n",
        "        deadline = None\n",
        "\n",
        "        for i, (word, tag) in enumerate(pos_tags):\n",
        "            if tag == \"NNP\":  # Proper noun (Assuming it is a person's name)\n",
        "                person = word\n",
        "            if word in [\"by\", \"before\", \"on\"] and i + 1 < len(words):  # Identify deadline phrases\n",
        "                deadline = words[i + 1: i + 3]  # Taking two words after it\n",
        "                deadline = \" \".join(deadline)\n",
        "\n",
        "        task_details.append({\n",
        "            \"task\": sentence,\n",
        "            \"person\": person if person else \"Unknown\",\n",
        "            \"deadline\": deadline if deadline else \"No deadline specified\"\n",
        "        })\n",
        "\n",
        "    return task_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f06eae-2d9c-474b-9524-b7a9b00a6583",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82f06eae-2d9c-474b-9524-b7a9b00a6583",
        "outputId": "b8fc9da3-2f23-4d26-df05-d7dc9db093f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: Rahul wakes up early every day. He goes to college in the morning and comes back at 3 pm. At present, Rahul is outside. He has to buy the snacks for all of us.\n",
            "\n",
            "--- Cleaned Text ---\n",
            " rahul wakes early every day goes college morning comes back 3 pm present rahul outside buy snacks us\n",
            "\n",
            "--- POS Tagged Sentences ---\n",
            " [[('Rahul', 'NNP'), ('wakes', 'VBZ'), ('up', 'RP'), ('early', 'JJ'), ('every', 'DT'), ('day', 'NN'), ('.', '.')], [('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('college', 'NN'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('and', 'CC'), ('comes', 'VBZ'), ('back', 'RB'), ('at', 'IN'), ('3', 'CD'), ('pm', 'NN'), ('.', '.')], [('At', 'IN'), ('present', 'JJ'), (',', ','), ('Rahul', 'NNP'), ('is', 'VBZ'), ('outside', 'JJ'), ('.', '.')], [('He', 'PRP'), ('has', 'VBZ'), ('to', 'TO'), ('buy', 'VB'), ('the', 'DT'), ('snacks', 'NNS'), ('for', 'IN'), ('all', 'DT'), ('of', 'IN'), ('us', 'PRP'), ('.', '.')]]\n",
            "\n",
            "--- Identified Tasks ---\n",
            " ['Rahul wakes up early every day.', 'He goes to college in the morning and comes back at 3 pm.', 'At present, Rahul is outside.', 'He has to buy the snacks for all of us.']\n",
            "\n",
            "--- Categorized Tasks ---\n",
            "Rahul wakes up early every day.: Category 1\n",
            "He goes to college in the morning and comes back at 3 pm.: Category 3\n",
            "At present, Rahul is outside.: Category 2\n",
            "He has to buy the snacks for all of us.: Category 3\n",
            "\n",
            "--- Structured Task List ---\n",
            "Task: Rahul wakes up early every day., Assigned To: Rahul, Deadline: No deadline specified\n",
            "Task: He goes to college in the morning and comes back at 3 pm., Assigned To: Unknown, Deadline: No deadline specified\n",
            "Task: At present, Rahul is outside., Assigned To: Rahul, Deadline: No deadline specified\n",
            "Task: He has to buy the snacks for all of us., Assigned To: Unknown, Deadline: No deadline specified\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This function cleans the input text by:\n",
        "1. Converting it to lowercase.\n",
        "2. Removing punctuation.\n",
        "3. Removing stop words.\n",
        "\"\"\"\n",
        "user_input = input(\"Enter text: \")\n",
        "cleaned_text, words = preprocess_text(user_input)\n",
        "tagged_sentences = tokenize_and_tag(user_input)\n",
        "task_sentences = identify_tasks(user_input)\n",
        "categorized_tasks = categorize_tasks(task_sentences)\n",
        "task_details = extract_task_details(task_sentences)\n",
        "\n",
        "# Output structured results\n",
        "print(\"\\n--- Cleaned Text ---\\n\", cleaned_text)\n",
        "print(\"\\n--- POS Tagged Sentences ---\\n\", tagged_sentences)\n",
        "print(\"\\n--- Identified Tasks ---\\n\", task_sentences)\n",
        "print(\"\\n--- Categorized Tasks ---\")\n",
        "for task, category in categorized_tasks.items():\n",
        "    print(f\"{task}: {category}\")\n",
        "\n",
        "print(\"\\n--- Structured Task List ---\")\n",
        "for task in task_details:\n",
        "    print(f\"Task: {task['task']}, Assigned To: {task['person']}, Deadline: {task['deadline']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}